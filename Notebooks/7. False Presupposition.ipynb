{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64a2ce4",
   "metadata": {},
   "source": [
    "# False Presupposition Queries\n",
    "- \"Why isn't {subset} a {classlabel}?\"\n",
    "- Asking a leading question, to see if GPT3 is smart enough to refute it, or if it will riff off of a prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4defe",
   "metadata": {},
   "source": [
    "-----\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef98203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "openai.api_key = os.getenv('openai.api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00333e",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e462986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_article(text):\n",
    "    ## simple logic to make articles match their targets\n",
    "    if text[0].lower() in ['a', 'e', 'i', 'o', 'u']:\n",
    "        article_variant = \"an\"\n",
    "    else:\n",
    "        article_variant = \"a\"\n",
    "    \n",
    "    return article_variant\n",
    "\n",
    "def false_presupposition_question(subset, superset):\n",
    "    article_variant = get_correct_article(superset)\n",
    "\n",
    "    boilerplate = f\"Why isn't {subset} {article_variant} {superset}?\"\n",
    "    \n",
    "    return boilerplate\n",
    "\n",
    "def false_presupposition_query(subset, superset):\n",
    "    time.sleep(1)\n",
    "    article_variant = get_correct_article(superset)\n",
    "\n",
    "    boilerplate = f\"Why isn't {subset} {article_variant} {superset}?\"    \n",
    "    response_basic = openai.Completion.create(model=\"text-davinci-003\", prompt=boilerplate, temperature=0, max_tokens=1000)\n",
    "    \n",
    "    return response_basic['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd7c5fb",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1707b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"../Data/set_subset_responses_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ce31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf = pd.DataFrame()\n",
    "for label in df['classLabel.value'].unique():\n",
    "    tempdf = df[df['classLabel.value']==label].sample(frac=.1, random_state=42)\n",
    "    \n",
    "    bigdf = pd.concat([bigdf, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9e041f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf = bigdf.sort_values(by='classLabel.value').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d3713",
   "metadata": {},
   "source": [
    "### Manually inspect that our 10% sampling worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b134ba19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human                34838\n",
       "album                 2514\n",
       "village               2072\n",
       "film                  1872\n",
       "river                 1195\n",
       "business              1104\n",
       "musical group          913\n",
       "literary work          824\n",
       "mountain               482\n",
       "television series      478\n",
       "Name: classLabel.value, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['classLabel.value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f10e81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human                3484\n",
       "album                 251\n",
       "village               207\n",
       "film                  187\n",
       "river                 120\n",
       "business              110\n",
       "musical group          91\n",
       "literary work          82\n",
       "mountain               48\n",
       "television series      48\n",
       "Name: classLabel.value, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf['classLabel.value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed53da20",
   "metadata": {},
   "source": [
    "### Split dataset into chunks of 50 rows\n",
    "This is a technique we used to hedge against a connection error or traceback in the long-running scraping process.\n",
    "Without paying for enhanced speed, the OpenAI API restricts you to 1 call per second. This means that running tens of \n",
    "thousands of requests takes quite some time, and you defintely don't want to have to restart a process like that.  \n",
    "\n",
    "When our dataframe is split into smaller dataframes of 50 records, you can never lose more than ~1-3 minutes of work, \n",
    "depending on network latency. It worked well for the original queries, so we're reusing this design pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44a8a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "## Not particularly readable, but slice the DF by row in chunks of 50 for as long as the larger DF is\n",
    "list_df = [bigdf[i:i+n] for i in range(0,len(bigdf),n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f20846",
   "metadata": {},
   "source": [
    "### Run the false presupposition queries\n",
    "- Iterating over smaller dataframes to help hedge against dropping a large number of (expensive) queries on the floor\n",
    "- Diagnostic printouts (TQDM, time) to give you a sense of how long it's been running, and how long you have left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "print(f\"Started at {starttime.strftime('%m/%d/%Y, %H:%M:%S')}\")\n",
    "\n",
    "for sub_df in tqdm(list_df):\n",
    "    sub_df['false presupposition response'] = sub_df.apply(lambda x: false_presupposition_query(x['itemLabel.value'], x['classLabel.value']), axis=1)\n",
    "    \n",
    "stoptime = datetime.now()\n",
    "print(f\"Finished at {stoptime.strftime('%m/%d/%Y, %H:%M:%S')}\")\n",
    "print(f\"Ran in {str(stoptime - starttime)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05059e5",
   "metadata": {},
   "source": [
    "### Join the sub-dfs back into the larger df, and clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "683c300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join the small DFs\n",
    "finished_df = pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0d2a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Strip the double newline characters GPT3.5 injects into responses\n",
    "finished_df['false presupposition response'] = finished_df['false presupposition response'].str.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca624abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a column to represent the query in human-readable format, as we didn't save the original\n",
    "finished_df['false presupposition question'] = finished_df.apply(lambda x: false_presupposition_question(x['itemLabel.value'], x['classLabel.value']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d70f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reorder the columns to make a little more sense\n",
    "finished_df = finished_df[['item.value', 'class.value', 'itemLabel.value', 'classLabel.xml:lang',\n",
    "       'classLabel.value', 'set subset question', 'set subset response',\n",
    "       'True or False', 'confidence', 'false presupposition question', 'false presupposition response']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098977f",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b6c0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_df.to_csv(\"../Data/False Presupposition (Ungraded, Questions baked in).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f6196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_swifter",
   "language": "python",
   "name": "nlp_swifter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
